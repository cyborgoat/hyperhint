HyperHint Development Notes
==========================

## Recent Changes
- Implemented SSE streaming for real-time chat responses
- Fixed processing dots issue - now shows in same bubble as content
- Added real LLM service integration (Ollama + OpenAI)
- Created memory system with file-based storage
- Updated long-term memory to focus on core actions

## Current Issues
- Need to add real file system scanning to short-term memory
- LLM services need proper error handling and fallbacks
- WebSocket suggestions could be more responsive
- File upload processing needs implementation

## Next Steps
1. Test LLM integrations with real Ollama/OpenAI services
2. Implement add_knowledge action to save user messages
3. Add file content reading capabilities
4. Improve error handling and user feedback
5. Add configuration management for API keys

## Technical Debt
- Mock data should be replaced with real file scanning
- Need proper logging system
- API rate limiting not implemented
- No authentication/authorization yet

## Performance Notes
- SSE streaming works well for chat responses
- Frontend handles large message histories efficiently
- Memory usage seems reasonable for current scale
- Consider pagination for large file lists

## Dependencies
- Backend: FastAPI, Pydantic, ollama, openai
- Frontend: Next.js 15, TypeScript, Tailwind, shadcn/ui
- Development: uv package manager, npm

## Environment Setup
- Python 3.8+ required
- Node.js 18+ required
- Ollama server for local LLM (optional)
- OpenAI API key for cloud LLM (optional) 